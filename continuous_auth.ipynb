{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "\n",
    "def load_files(directory: str) -> list[str]:\n",
    "    file_paths = np.array([])\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                file_paths = np.append(file_paths, os.path.join(root, file))\n",
    "    return file_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "timestamp = 'timestamp'\n",
    "unix_timestamp = 'unix_timestamp'\n",
    "\n",
    "def load_data(file0: str, file1:str, label_key:str) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    normals = pd.read_csv(file0, encoding='latin-1') \n",
    "    anomalies = pd.read_csv(file1, encoding='latin-1') \n",
    "    normals[label_key] = 0\n",
    "    anomalies[label_key] = 1\n",
    "    normals[unix_timestamp], normals[timestamp] = normals[timestamp], pd.to_datetime(normals[timestamp], unit='ms')\n",
    "    anomalies[unix_timestamp], anomalies[timestamp] = anomalies[timestamp], pd.to_datetime(anomalies[timestamp], unit='ms')\n",
    "    print(f'loaded normals: {normals.shape},  loaded anomalies: {anomalies.shape}')\n",
    "    return normals, anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  datetime import timedelta, datetime\n",
    "\n",
    "def random_time_range(anomalies: pd.DataFrame, iteration: int) -> tuple[datetime, datetime]:\n",
    "    #determine start and end time. subtrat one second from start time to ensure the real start time is always included\n",
    "    start_time, end_time = anomalies[timestamp].min() - timedelta(seconds=1) , anomalies[timestamp].max()\n",
    "    print(f'anomalies start_time: {start_time} - end_time: {end_time}')\n",
    "    \n",
    "    #pick a random time delta between a range, weighted by the number of iterations\n",
    "    delta = timedelta(minutes= np.random.randint(5,30) + 2 ** iteration)\n",
    "    latest_start  = np.maximum(end_time - delta, start_time)\n",
    "        \n",
    "    random_start = start_time + timedelta(seconds=np.random.randint(0, int((latest_start - start_time).total_seconds() + 1))) \n",
    "    random_end = random_start + delta\n",
    "    print(f'random time rand selected {random_start} - {random_end}')\n",
    "    return random_start, random_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inject_anomalies(normal:pd.DataFrame, anomalies:pd.DataFrame, start_time: datetime, end_time:datetime) -> tuple[pd.DataFrame, pd.Index]:\n",
    "    anomalies_condition = (start_time <= anomalies[timestamp]) & (anomalies[timestamp] <= end_time)\n",
    "    normal_conditoin = (start_time <= normal[timestamp]) & (normal[timestamp] <= end_time)\n",
    "    #find anomalies that fall within the random time range\n",
    "    subset_anomalies = anomalies[anomalies_condition]\n",
    "    index = subset_anomalies.index\n",
    "    print(f'{subset_anomalies.shape[0]} anomalies selected')\n",
    "\n",
    "    #if no anomalies found do nothing\n",
    "    if subset_anomalies.empty:\n",
    "        return normal, index\n",
    "    \n",
    "    #find normals that fall within the same random time range\n",
    "    subset_normal = normal[normal_conditoin]\n",
    "    \n",
    "    #if no normals found append anomalies to normals list\n",
    "    if subset_normal.empty:\n",
    "        print('no normal data within range, appending anomalies')\n",
    "        return pd.concat([normal,subset_anomalies], ignore_index=True), index\n",
    "    \n",
    "    #if normals found remove them and append anomalies to list to prevent unrealistic data where 2 users are interating with 1 machine\n",
    "    print(f'{subset_normal.shape[0]} normals selected to be overwritten')\n",
    "    \n",
    "    return pd.concat([normal.drop(normal[(subset_anomalies[timestamp].min() <= normal[timestamp]) & (subset_anomalies[timestamp].max() <= end_time)].index), subset_anomalies], ignore_index=True), index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(normals:pd.DataFrame, anomalies:pd.DataFrame, label_key:str, anomaly_percentage_target:float=0.05 ) -> tuple[pd.DataFrame,float]:\n",
    "    max_size = normals.shape[0] + anomalies.shape[0]\n",
    "    anomaly_percentage = 0\n",
    "    iteration = 0\n",
    "    #merge two datasets\n",
    "    while anomaly_percentage < anomaly_percentage_target and normals.shape[0] < max_size and not anomalies.empty:\n",
    "        iteration+=1\n",
    "        print(f'current iteration: {iteration} - anomaly_size: {anomalies.shape} - anomaly_percentage: {anomaly_percentage} - current_size: {normals.shape[0]} -  max_size: {max_size}')\n",
    "        print(f'starting shape: {normals.shape}')\n",
    "        normals, index = inject_anomalies(normals, anomalies, *random_time_range(anomalies, iteration))\n",
    "        anomalies.drop(index, inplace=True)\n",
    "        print(f'ending shape: {normals.shape}')\n",
    "        anomaly_percentage = normals[normals[label_key] == 1].shape[0] / normals.shape[0]\n",
    "\n",
    "    print(f'total iterations: {iteration} - final anomaly_percentage: {anomaly_percentage}')\n",
    "    normals.sort_values(timestamp)\n",
    "    print(f'merged shape: {normals.shape}')\n",
    "    return normals, anomaly_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def reduce_dimensions(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    columns_to_drop = [\n",
    "        timestamp,\n",
    "        # 'active_apps_average',\n",
    "        # 'current_app',\n",
    "        # 'penultimate_app',\n",
    "        # 'changes_between_apps',\n",
    "        # 'current_app_foreground_time',\n",
    "        # 'current_app_average_processes',\n",
    "        # 'current_app_stddev_processes',\n",
    "        # 'current_app_average_cpu',\n",
    "        # 'current_app_stddev_cpu',\n",
    "        # 'system_average_cpu',\n",
    "        # 'system_stddev_cpu',\n",
    "        # 'current_app_average_mem',\n",
    "        # 'current_app_stddev_mem',\n",
    "        # 'system_average_mem',\n",
    "        # 'system_stddev_mem',\n",
    "        # 'received_bytes',\n",
    "        # 'sent_bytes',\n",
    "        'USER'\n",
    "    ]\n",
    "    return data.iloc[:, ~data.columns.isin(columns_to_drop)]\n",
    "\n",
    "def encode_columns(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    string_columns = data.select_dtypes(include=['object'])\n",
    "    for column in string_columns:\n",
    "        label_encoder = LabelEncoder()\n",
    "        print(f\"encoding column: {column}\")\n",
    "        data.loc[:,column] = label_encoder.fit_transform(data[column])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def normalize_data(data: pd.DataFrame, label_key: str) -> tuple[np.ndarray,np.ndarray]:\n",
    "    user_mask = data.columns.isin([label_key])\n",
    "    features = data.iloc[:, ~user_mask].to_numpy()\n",
    "    labels = data.iloc[:, user_mask].to_numpy()\n",
    "    \n",
    "    #normalizing data \n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "    return features_scaled, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "def train_test_validate_split_data(features: np.ndarray, labels: np.ndarray, train_size: float = 0.70, test_size: float = 0.15, validate_size: float = 0.15, look_back: int = 0, batch_size: int = 128) -> tuple[TimeseriesGenerator,TimeseriesGenerator,TimeseriesGenerator]:\n",
    "\n",
    "    #split data\n",
    "    non_train_size = test_size + validate_size\n",
    "    features_train, features_temp, labels_train, labels_temp = train_test_split(features,labels, train_size=train_size, test_size= non_train_size, shuffle=True, random_state=42) \n",
    "\n",
    "    # Split the temporary data into 50% validation and 50% test (50% of 30% is 15% each)\n",
    "    features_validation, features_test, labels_validation, labels_test = train_test_split(features_temp, labels_temp, train_size= validate_size / non_train_size, test_size=test_size / non_train_size, shuffle=False, random_state=42)\n",
    "    \n",
    "    train_generator = TimeseriesGenerator(features_train, labels_train, length=look_back, batch_size=batch_size)\n",
    "    test_generator = TimeseriesGenerator(features_test, labels_test, length=look_back, batch_size=batch_size)\n",
    "    validation_generator = TimeseriesGenerator(features_validation, labels_validation, length=look_back, batch_size=batch_size)\n",
    "    return train_generator, test_generator, validation_generator \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,mean_absolute_error, root_mean_squared_error, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def safe_divide(numerator: float, denomonator: float) -> float:\n",
    "    return 0 if denomonator == 0 else numerator / denomonator\n",
    "\n",
    "class Results:\n",
    "    def __init__(self, actual, predictions):\n",
    "        self.accuracy = accuracy_score(actual, predictions)\n",
    "        self.mae = mean_absolute_error(actual, predictions)\n",
    "        self.rmse = root_mean_squared_error(actual, predictions)\n",
    "        self.cm = confusion_matrix(actual, predictions, labels=[True, False])\n",
    "        self.true_negatives, self.false_positives, self.false_negatives, self.true_positives = self.cm.ravel()\n",
    "        self.total = self.true_negatives + self.false_positives + self.false_negatives + self.true_positives\n",
    "        self.false_positives_rate = self.false_positives / self.total\n",
    "        self.false_negatives_rate = self.false_negatives / self.total\n",
    "        self.precision = safe_divide(self.true_positives, (self.true_positives + self.false_positives) )  \n",
    "        self.recall = safe_divide(self.true_positives, (self.true_positives + self.false_negatives))  \n",
    "        self.f1_score =  2 * safe_divide((self.precision * self.recall) ,(self.precision + self.recall))\n",
    "\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'''Results \n",
    "    Accuracy: {self.accuracy:.2f}\n",
    "    Mean Absolute Error: {self.mae:.2f}\n",
    "    Root Mean Squared Error: {self.rmse:.2f}\n",
    "    False Positives: {self.false_positives_rate:.2f}\n",
    "    Fales Negatives: {self.false_negatives_rate: .2f}\n",
    "    Precision: {self.precision:.2f}\n",
    "    Recall: {self.recall:.2f}\n",
    "    F1 Score: {self.f1_score:.2f}'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Dropout, Bidirectional\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from tensorflow.keras.metrics import Recall, Precision, TruePositives, TrueNegatives, FalseNegatives, FalsePositives, F1Score\n",
    "\n",
    "\n",
    "def build_train_test_validate_model(train: TimeseriesGenerator, test:TimeseriesGenerator,  validation: TimeseriesGenerator, epochs:int, look_back:int=0):\n",
    "    \n",
    "    #build model\n",
    "    model = Sequential()\n",
    "    features_count = train.data.shape[1]\n",
    "    model.add(Input(shape=(look_back,features_count)))\n",
    "    model.add(Bidirectional(LSTM(50, kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu')))\n",
    "    # model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    learning_rate = 0.0001\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    accuracy = BinaryAccuracy(name=\"binary_accuracy\", dtype=int, threshold=0.5)\n",
    "    metrics = [\n",
    "        accuracy,\n",
    "        Recall(),\n",
    "        Precision(),\n",
    "        TruePositives(),\n",
    "        TrueNegatives(),\n",
    "        FalseNegatives(),\n",
    "        FalsePositives(),\n",
    "        F1Score()\n",
    "    ]\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=metrics)\n",
    "    model.summary()\n",
    "    \n",
    "    #train and test model\n",
    "    history = model.fit(train, steps_per_epoch=len(train), epochs=epochs, verbose=1, validation_data=test)\n",
    "    \n",
    "    #validate model\n",
    "    predictions = model.predict(validation)\n",
    "    predictions = (predictions >= 0.5).astype(int)\n",
    "    actual = validation.targets[look_back:]\n",
    "    results = Results(actual, predictions)\n",
    "\n",
    "    return history, results  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plotHistory(axes, data, history):\n",
    "    anomaly_percentage = f'{data[\"anomaly_percentage\"]:1.2%}'\n",
    "    plt1 = axes[0,0]\n",
    "    plt1.plot(history['loss'], label=anomaly_percentage)\n",
    "    plt1.set_title(f'Training Loss')\n",
    "    plt1.set_ylabel('Loss')\n",
    "    \n",
    "    plt2 = axes[0,1]\n",
    "    plt2.plot(history['val_loss'], label=anomaly_percentage)\n",
    "    plt2.set_title(f'Validation Loss')\n",
    "    \n",
    "    plt3 = axes[1,0]\n",
    "    plt3.plot(history['binary_accuracy'], label=anomaly_percentage)\n",
    "    plt3.set_title(f'Training Accuracy')\n",
    "    plt3.set_ylabel('Accuracy')\n",
    "    \n",
    "    plt4 = axes[1,1]\n",
    "    plt4.plot(history['val_binary_accuracy'], label=anomaly_percentage)\n",
    "    plt4.set_title(f'Validation Accuracy')\n",
    "        \n",
    "\n",
    "def plotConfusionMatrix(matrix):\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=['Normal', 'Anomaly'])\n",
    "    disp.plot()\n",
    "\n",
    "def plotMetrics(metrics: list):\n",
    "    fig, axes = plt.subplots(2, 2, sharex=True, sharey='row')\n",
    "    fig.supxlabel('Epoch')\n",
    "\n",
    "    for  metric in metrics:\n",
    "        plotHistory(axes, metric['merged'], metric['history'])\n",
    "        plotConfusionMatrix(metric['results'].cm)\n",
    "        \n",
    "    handles, labels = fig.axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper center',title='Anomaly Percentage' )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changed working dir to: /Users/osono/Library/CloudStorage/OneDrive-NorthCarolinaA&TStateUniversity/COMP 850 - 1A Big Data Analytics/Project/continuous authentication/datasets/Behacom\n",
      "loaded normals: (17284, 12053),  loaded anomalies: (2128, 12053)\n",
      "current iteration: 1 - anomaly_size: (2128, 12053) - anomaly_percentage: 0 - current_size: 17284 -  max_size: 19412\n",
      "starting shape: (17284, 12053)\n",
      "anomalies start_time: 2019-12-02 20:51:43.401000 - end_time: 2019-12-06 11:08:33.639000\n",
      "random time rand selected 2019-12-03 08:27:52.401000 - 2019-12-03 08:46:52.401000\n",
      "0 anomalies selected\n",
      "ending shape: (17284, 12053)\n",
      "current iteration: 2 - anomaly_size: (2128, 12053) - anomaly_percentage: 0.0 - current_size: 17284 -  max_size: 19412\n",
      "starting shape: (17284, 12053)\n",
      "anomalies start_time: 2019-12-02 20:51:43.401000 - end_time: 2019-12-06 11:08:33.639000\n",
      "random time rand selected 2019-12-04 09:46:44.401000 - 2019-12-04 09:57:44.401000\n",
      "0 anomalies selected\n",
      "ending shape: (17284, 12053)\n",
      "current iteration: 3 - anomaly_size: (2128, 12053) - anomaly_percentage: 0.0 - current_size: 17284 -  max_size: 19412\n",
      "starting shape: (17284, 12053)\n",
      "anomalies start_time: 2019-12-02 20:51:43.401000 - end_time: 2019-12-06 11:08:33.639000\n",
      "random time rand selected 2019-12-04 20:06:59.401000 - 2019-12-04 20:38:59.401000\n",
      "0 anomalies selected\n",
      "ending shape: (17284, 12053)\n",
      "current iteration: 4 - anomaly_size: (2128, 12053) - anomaly_percentage: 0.0 - current_size: 17284 -  max_size: 19412\n",
      "starting shape: (17284, 12053)\n",
      "anomalies start_time: 2019-12-02 20:51:43.401000 - end_time: 2019-12-06 11:08:33.639000\n",
      "random time rand selected 2019-12-04 04:30:06.401000 - 2019-12-04 04:53:06.401000\n",
      "0 anomalies selected\n",
      "ending shape: (17284, 12053)\n",
      "current iteration: 5 - anomaly_size: (2128, 12053) - anomaly_percentage: 0.0 - current_size: 17284 -  max_size: 19412\n",
      "starting shape: (17284, 12053)\n",
      "anomalies start_time: 2019-12-02 20:51:43.401000 - end_time: 2019-12-06 11:08:33.639000\n",
      "random time rand selected 2019-12-04 14:37:23.401000 - 2019-12-04 15:27:23.401000\n",
      "48 anomalies selected\n",
      "no normal data within range, appending anomalies\n",
      "ending shape: (17332, 12053)\n",
      "current iteration: 6 - anomaly_size: (2080, 12053) - anomaly_percentage: 0.0027694438033694898 - current_size: 17332 -  max_size: 19412\n",
      "starting shape: (17332, 12053)\n",
      "anomalies start_time: 2019-12-02 20:51:43.401000 - end_time: 2019-12-06 11:08:33.639000\n",
      "random time rand selected 2019-12-04 03:03:40.401000 - 2019-12-04 04:18:40.401000\n",
      "0 anomalies selected\n",
      "ending shape: (17332, 12053)\n",
      "current iteration: 7 - anomaly_size: (2080, 12053) - anomaly_percentage: 0.0027694438033694898 - current_size: 17332 -  max_size: 19412\n",
      "starting shape: (17332, 12053)\n",
      "anomalies start_time: 2019-12-02 20:51:43.401000 - end_time: 2019-12-06 11:08:33.639000\n",
      "random time rand selected 2019-12-03 19:57:26.401000 - 2019-12-03 22:32:26.401000\n",
      "148 anomalies selected\n",
      "93 normals selected to be overwritten\n",
      "ending shape: (5155, 12053)\n",
      "current iteration: 8 - anomaly_size: (1932, 12053) - anomaly_percentage: 0.02870999030067895 - current_size: 5155 -  max_size: 19412\n",
      "starting shape: (5155, 12053)\n",
      "anomalies start_time: 2019-12-02 20:51:43.401000 - end_time: 2019-12-06 11:08:33.639000\n",
      "random time rand selected 2019-12-03 21:50:13.401000 - 2019-12-04 02:32:13.401000\n",
      "124 anomalies selected\n",
      "41 normals selected to be overwritten\n",
      "ending shape: (5279, 12053)\n",
      "total iterations: 8 - final anomaly_percentage: 0.051524910020837277\n",
      "merged shape: (5279, 12053)\n",
      "encoding column: current_app\n",
      "encoding column: penultimate_app\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,840,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">101</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │     \u001b[38;5;34m4,840,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m101\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,840,501</span> (18.47 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,840,501\u001b[0m (18.47 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,840,501</span> (18.47 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,840,501\u001b[0m (18.47 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    }
   ],
   "source": [
    "working_dir = '/Users/osono/Library/CloudStorage/OneDrive-NorthCarolinaA&TStateUniversity/COMP 850 - 1A Big Data Analytics/Project/continuous authentication/datasets/Behacom'\n",
    "os.chdir(working_dir)\n",
    "current_directory = os.getcwd()\n",
    "print(f'changed working dir to: {current_directory}')\n",
    "\n",
    "label_key = 'is_anomaly'\n",
    "look_back = 3\n",
    "epochs = 5\n",
    "file_paths = load_files(current_directory)\n",
    "\n",
    "metrics = []\n",
    "while file_paths.size > 0 and len(metrics)  < 1:\n",
    "    choices =  np.random.choice(file_paths, size=2, replace=False)\n",
    "    file_paths = file_paths[np.isin(file_paths, choices, invert=True)]\n",
    "    normals, anomalies = load_data(choices[0], choices[1], label_key)\n",
    "    metric = {\n",
    "        'normals': {\n",
    "            'path': choices[0],\n",
    "            'shape': normals.shape\n",
    "        },\n",
    "        'anomalies': {\n",
    "            'path': choices[1],\n",
    "            'shape': anomalies.shape\n",
    "        }\n",
    "    }\n",
    "    merged, anomaly_percentage = merge_data(normals, anomalies, label_key)\n",
    "    merged = encode_columns(reduce_dimensions(merged))\n",
    "    metric['merged'] = {\n",
    "        'shape': merged.shape,\n",
    "        'anomaly_percentage': anomaly_percentage\n",
    "    }\n",
    "    \n",
    "    features, labels = normalize_data(merged, label_key)\n",
    "    train, test, validation = train_test_validate_split_data(features, labels, look_back=look_back, batch_size=10)\n",
    "    history, results = build_train_test_validate_model(train, test, validation, epochs=epochs, look_back=look_back)\n",
    "    metric['history'] = history.history\n",
    "    metric['results'] = results\n",
    "    metrics.append(metric)\n",
    "\n",
    "plotMetrics(metrics)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
