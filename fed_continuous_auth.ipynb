{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "\n",
    "def load_files(directory: str) -> list[str]:\n",
    "    file_paths = np.array([])\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                file_paths = np.append(file_paths, os.path.join(root, file))\n",
    "    return file_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "timestamp = 'timestamp'\n",
    "unix_timestamp = 'unix_timestamp'\n",
    "\n",
    "def load_data(file0: str, file1:str, label_key:str) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    normals = pd.read_csv(file0, encoding='latin-1') \n",
    "    anomalies = pd.read_csv(file1, encoding='latin-1') \n",
    "    normals[label_key] = 0\n",
    "    anomalies[label_key] = 1\n",
    "    normals[unix_timestamp], normals[timestamp] = normals[timestamp], pd.to_datetime(normals[timestamp], unit='ms')\n",
    "    anomalies[unix_timestamp], anomalies[timestamp] = anomalies[timestamp], pd.to_datetime(anomalies[timestamp], unit='ms')\n",
    "    print(f'loaded normals: {normals.shape},  loaded anomalies: {anomalies.shape}')\n",
    "    return normals, anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  datetime import timedelta, datetime\n",
    "\n",
    "def random_time_range(anomalies: pd.DataFrame, iteration: int) -> tuple[datetime, datetime]:\n",
    "    #determine start and end time. subtrat one second from start time to ensure the real start time is always included\n",
    "    start_time, end_time = anomalies[timestamp].min() - timedelta(seconds=1) , anomalies[timestamp].max()\n",
    "    print(f'anomalies start_time: {start_time} - end_time: {end_time}')\n",
    "    \n",
    "    #pick a random time delta between a range, weighted by the number of iterations\n",
    "    delta = timedelta(minutes= np.random.randint(5,30) + 2 ** iteration)\n",
    "    latest_start  = np.maximum(end_time - delta, start_time)\n",
    "        \n",
    "    random_start = start_time + timedelta(seconds=np.random.randint(0, int((latest_start - start_time).total_seconds() + 1))) \n",
    "    random_end = random_start + delta\n",
    "    print(f'random time rand selected {random_start} - {random_end}')\n",
    "    return random_start, random_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inject_anomalies(normal:pd.DataFrame, anomalies:pd.DataFrame, start_time: datetime, end_time:datetime) -> tuple[pd.DataFrame, pd.Index]:\n",
    "    anomalies_condition = (start_time <= anomalies[timestamp]) & (anomalies[timestamp] <= end_time)\n",
    "    normal_conditoin = (start_time <= normal[timestamp]) & (normal[timestamp] <= end_time)\n",
    "    #find anomalies that fall within the random time range\n",
    "    subset_anomalies = anomalies[anomalies_condition]\n",
    "    index = subset_anomalies.index\n",
    "    print(f'{subset_anomalies.shape[0]} anomalies selected')\n",
    "\n",
    "    #if no anomalies found do nothing\n",
    "    if subset_anomalies.empty:\n",
    "        return normal, index\n",
    "    \n",
    "    #find normals that fall within the same random time range\n",
    "    subset_normal = normal[normal_conditoin]\n",
    "    \n",
    "    #if no normals found append anomalies to normals list\n",
    "    if subset_normal.empty:\n",
    "        print('no normal data within range, appending anomalies')\n",
    "        return pd.concat([normal,subset_anomalies], ignore_index=True), index\n",
    "    \n",
    "    #if normals found remove them and append anomalies to list to prevent unrealistic data where 2 users are interating with 1 machine\n",
    "    print(f'{subset_normal.shape[0]} normals selected to be overwritten')\n",
    "    \n",
    "    return pd.concat([normal.drop(normal[(subset_anomalies[timestamp].min() <= normal[timestamp]) & (subset_anomalies[timestamp].max() <= end_time)].index), subset_anomalies], ignore_index=True), index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(normals:pd.DataFrame, anomalies:pd.DataFrame, label_key:str, anomaly_percentage_target:float=0.05 ) -> tuple[pd.DataFrame,float]:\n",
    "    max_size = normals.shape[0] + anomalies.shape[0]\n",
    "    anomaly_percentage = 0\n",
    "    iteration = 0\n",
    "    #merge two datasets\n",
    "    while anomaly_percentage < anomaly_percentage_target and normals.shape[0] < max_size and not anomalies.empty:\n",
    "        iteration+=1\n",
    "        print(f'current iteration: {iteration} - anomaly_size: {anomalies.shape} - anomaly_percentage: {anomaly_percentage} - current_size: {normals.shape[0]} -  max_size: {max_size}')\n",
    "        print(f'starting shape: {normals.shape}')\n",
    "        normals, index = inject_anomalies(normals, anomalies, *random_time_range(anomalies, iteration))\n",
    "        anomalies.drop(index, inplace=True)\n",
    "        print(f'ending shape: {normals.shape}')\n",
    "        anomaly_percentage = normals[normals[label_key] == 1].shape[0] / normals.shape[0]\n",
    "\n",
    "    print(f'total iterations: {iteration} - final anomaly_percentage: {anomaly_percentage}')\n",
    "    normals.sort_values(timestamp)\n",
    "    print(f'merged shape: {normals.shape}')\n",
    "    return normals, anomaly_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def reduce_dimensions(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    columns_to_drop = [\n",
    "        timestamp,\n",
    "        # 'active_apps_average',\n",
    "        # 'current_app',\n",
    "        # 'penultimate_app',\n",
    "        # 'changes_between_apps',\n",
    "        # 'current_app_foreground_time',\n",
    "        # 'current_app_average_processes',\n",
    "        # 'current_app_stddev_processes',\n",
    "        # 'current_app_average_cpu',\n",
    "        # 'current_app_stddev_cpu',\n",
    "        # 'system_average_cpu',\n",
    "        # 'system_stddev_cpu',\n",
    "        # 'current_app_average_mem',\n",
    "        # 'current_app_stddev_mem',\n",
    "        # 'system_average_mem',\n",
    "        # 'system_stddev_mem',\n",
    "        # 'received_bytes',\n",
    "        # 'sent_bytes',\n",
    "        'USER'\n",
    "    ]\n",
    "    return data.iloc[:, ~data.columns.isin(columns_to_drop)]\n",
    "\n",
    "def encode_columns(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    string_columns = data.select_dtypes(include=['object'])\n",
    "    for column in string_columns:\n",
    "        label_encoder = LabelEncoder()\n",
    "        print(\"encoding: {column}\")\n",
    "        data[column] = label_encoder.fit_transform(data[column])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def normalize_data(data: pd.DataFrame, label_key: str) -> tuple[np.ndarray,np.ndarray]:\n",
    "    user_mask = data.columns.isin([label_key])\n",
    "    features = data.iloc[:, ~user_mask].to_numpy()\n",
    "    labels = data.iloc[:, user_mask].to_numpy()\n",
    "    \n",
    "    #normalizing data \n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "    return features_scaled, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np \n",
    "from flwr_datasets import FederatedDataset\n",
    "\n",
    "timestamp = 'timestamp'\n",
    "unix_timestamp = 'unix_timestamp'\n",
    "label_key = 'is_anomaly'\n",
    "\n",
    "\n",
    "# Custom Federated Dataset\n",
    "class BehacomFederatedDataset(FederatedDataset):\n",
    "    def __init__(self, directory:str, label_key:str):\n",
    "        self.label_key = label_key\n",
    "        self.file_paths = load_files(directory)\n",
    "        self.total_files = len(self.file_paths)\n",
    "        if self.total_files == 0:\n",
    "            raise ValueError(f\"No files loaded: {directory}\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the dataset for a specific client\n",
    "        j = np.random.choice([i for i in range(0, self.total_files) if i != idx - 1])\n",
    "        benign_path, anomalous_path = self.file_paths[idx - 1], self.file_paths[j]\n",
    "        benign, anomalous = load_data(benign_path, anomalous_path, label_key)\n",
    "        merged, anomaly_percentage = merge_data(benign, anomalous, label_key)\n",
    "        return encode_columns(reduce_dimensions(merged))\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of clients\n",
    "        return len(self.client_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "def train_test_validate_split_data(features: np.ndarray, labels: np.ndarray, train_size: float = 0.70, test_size: float = 0.15, validate_size: float = 0.15, look_back: int = 0, batch_size: int = 128) -> tuple[TimeseriesGenerator,TimeseriesGenerator,TimeseriesGenerator]:\n",
    "    #split data\n",
    "    non_train_size = test_size + validate_size\n",
    "    features_train, features_temp, labels_train, labels_temp = train_test_split(features,labels, train_size=train_size, test_size= non_train_size, shuffle=True, random_state=42) \n",
    "\n",
    "    # Split the temporary data into 50% validation and 50% test (50% of 30% is 15% each)\n",
    "    features_validation, features_test, labels_validation, labels_test = train_test_split(features_temp, labels_temp, train_size= validate_size / non_train_size, test_size=test_size / non_train_size, shuffle=False, random_state=42)\n",
    "    \n",
    "    train_generator = TimeseriesGenerator(features_train, labels_train, length=look_back, batch_size=batch_size)\n",
    "    test_generator = TimeseriesGenerator(features_test, labels_test, length=look_back, batch_size=batch_size)\n",
    "    validation_generator = TimeseriesGenerator(features_validation, labels_validation, length=look_back, batch_size=batch_size)\n",
    "    return train_generator, test_generator, validation_generator \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = {\n",
    "    \"look-back\" : 3,\n",
    "    \"learning-rate\" : 0.01,\n",
    "    \"label-key\" : \"is_anomolous\",\n",
    "    \"train-percentage\" : 0.7,\n",
    "    \"local-epochs\" : 3,\n",
    "    \"num-server-rounds\" : 3,\n",
    "    \"batch-size\" : 10,\n",
    "    \"verbose\" : False,\n",
    "    \"data-dir\" : \"datasets/Behacom\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Dropout, Bidirectional\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "\n",
    "def load_model(features_count, look_back, learning_rate):\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Input(shape=(look_back, features_count)),\n",
    "            Bidirectional(\n",
    "                    LSTM(50, kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu')\n",
    "            ),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ]\n",
    "    )\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    accuracy = BinaryAccuracy(name=\"binary_accuracy\", dtype=int, threshold=0.5)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[accuracy])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fds = None  # Cache FederatedDataset\n",
    "\n",
    "def load_client_data(dataset_dir:str, client_idx:int, label_key: str, train_size: float = 0.70, test_size: float = 0.15, validate_size: float = 0.15, look_back: int = 0, batch_size: int = 128):\n",
    "    global fds\n",
    "    if fds is None:\n",
    "        fds = BehacomFederatedDataset(dataset_dir,label_key)\n",
    "\n",
    "    data = fds[client_idx]\n",
    "    features, labels = normalize_data(data, label_key)\n",
    "    \n",
    "    #split data\n",
    "    non_train_size = test_size + validate_size\n",
    "    features_train, features_temp, labels_train, labels_temp = train_test_split(features,labels, train_size=train_size, test_size= non_train_size, shuffle=True, random_state=42) \n",
    "\n",
    "    # Split the temporary data into 50% validation and 50% test (50% of 30% is 15% each)\n",
    "    features_validation, features_test, labels_validation, labels_test = train_test_split(features_temp, labels_temp, train_size= validate_size / non_train_size, test_size=test_size / non_train_size, shuffle=False, random_state=42)\n",
    "    \n",
    "    train_generator = TimeseriesGenerator(features_train, labels_train, length=look_back, batch_size=batch_size)\n",
    "    test_generator = TimeseriesGenerator(features_test, labels_test, length=look_back, batch_size=batch_size)\n",
    "    validation_generator = TimeseriesGenerator(features_validation, labels_validation, length=look_back, batch_size=batch_size)\n",
    "    return train_generator, test_generator, validation_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flwr.client import NumPyClient, ClientApp\n",
    "from flwr.common import Context\n",
    "\n",
    "# Define Flower Client and client_fn\n",
    "class FlowerClient(NumPyClient):\n",
    "    def __init__(\n",
    "        self, model, data\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.train, self.test, self.validation = data\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "        self.model.fit(\n",
    "            self.train,\n",
    "            epochs=config['local-epochs'],\n",
    "            batch_size=config['batch-size'],\n",
    "            verbose=config['verbose'],\n",
    "            validation_data=self.test\n",
    "        )\n",
    "        return self.model.get_weights(), len(self.train), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "        loss, accuracy = self.model.evaluate(self.validation, verbose=0)\n",
    "        return loss, len(self.x_test), {\"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "def client_fn(context: Context):\n",
    "    # Load model and data\n",
    "    look_back = run_config[\"look-back\"]\n",
    "    learning_rate = run_config[\"learning-rate\"]\n",
    "\n",
    "\n",
    "    net = load_model(12034, look_back,learning_rate)\n",
    "\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "    label_key = run_config[\"label-key\"]\n",
    "    train_size = run_config[\"train-percentage\"]\n",
    "    batch_size = run_config[\"batch-size\"]\n",
    "    data_dir = run_config[\"data-dir\"]\n",
    "    data = load_client_data(data_dir, partition_id-1, label_key, train_size, look_back=look_back, batch_size=batch_size)\n",
    "\n",
    "    epochs = run_config[\"local-epochs\"]\n",
    "    verbose = run_config.get(\"verbose\")\n",
    "\n",
    "    # Return Client instance\n",
    "    return FlowerClient(\n",
    "        net, data\n",
    "    ).to_client()\n",
    "\n",
    "\n",
    "# Flower ClientApp\n",
    "client = ClientApp(\n",
    "    client_fn=client_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"federated-continuous-auth: A Flower / TensorFlow app.\"\"\"\n",
    "\n",
    "from flwr.common import Context, ndarrays_to_parameters\n",
    "from flwr.server import ServerApp, ServerAppComponents, ServerConfig\n",
    "from flwr.server.strategy import FedAvg\n",
    "\n",
    "def server_fn(context: Context):\n",
    "    # Read from config\n",
    "    num_rounds = run_config[\"num-server-rounds\"]\n",
    "\n",
    "    # Get parameters to initialize global model\n",
    "    look_back = run_config[\"look-back\"]\n",
    "    learning_rate = run_config[\"learning-rate\"]\n",
    "    #TODO: pull shape dinamically\n",
    "    parameters = ndarrays_to_parameters(load_model(12034, look_back, learning_rate).get_weights())\n",
    "\n",
    "    def fit_config(server_round: int):\n",
    "        run_config['current_round'] = server_round\n",
    "        return run_config\n",
    "    \n",
    "    # Define strategy\n",
    "    strategy = strategy = FedAvg(\n",
    "        fraction_fit=1.0,\n",
    "        fraction_evaluate=1.0,\n",
    "        min_available_clients=2,\n",
    "        initial_parameters=parameters,\n",
    "        on_fit_config_fn=fit_config\n",
    "    )\n",
    "    config = ServerConfig(num_rounds=num_rounds)\n",
    "\n",
    "    return ServerAppComponents(strategy=strategy, config=config)\n",
    "\n",
    "# Create ServerApp\n",
    "server = ServerApp(server_fn=server_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flwr.simulation import run_simulation\n",
    "import os\n",
    "\n",
    "run_simulation(\n",
    "    server_app=server,\n",
    "    client_app=client,\n",
    "    num_supernodes=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,mean_absolute_error, root_mean_squared_error, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def safe_divide(numerator: float, denomonator: float) -> float:\n",
    "    return 0 if denomonator == 0 else numerator / denomonator\n",
    "\n",
    "class Results:\n",
    "    def __init__(self, actual, predictions):\n",
    "        self.accuracy = accuracy_score(actual, predictions)\n",
    "        self.mae = mean_absolute_error(actual, predictions)\n",
    "        self.rmse = root_mean_squared_error(actual, predictions)\n",
    "        self.cm = confusion_matrix(actual, predictions, labels=[True, False])\n",
    "        self.true_negatives, self.false_positives, self.false_negatives, self.true_positives = self.cm.ravel()\n",
    "        self.total = self.true_negatives + self.false_positives + self.false_negatives + self.true_positives\n",
    "        self.false_positives_rate = self.false_positives / self.total\n",
    "        self.false_negatives_rate = self.false_negatives / self.total\n",
    "        self.precision = safe_divide(self.true_positives, (self.true_positives + self.false_positives) )  \n",
    "        self.recall = safe_divide(self.true_positives, (self.true_positives + self.false_negatives))  \n",
    "        self.f1_score =  2 * safe_divide((self.precision * self.recall) ,(self.precision + self.recall))\n",
    "\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'''Results \n",
    "    Accuracy: {self.accuracy:.2f}\n",
    "    Mean Absolute Error: {self.mae:.2f}\n",
    "    Root Mean Squared Error: {self.rmse:.2f}\n",
    "    False Positives: {self.false_positives_rate:.2f}\n",
    "    Fales Negatives: {self.false_negatives_rate: .2f}\n",
    "    Precision: {self.precision:.2f}\n",
    "    Recall: {self.recall:.2f}\n",
    "    F1 Score: {self.f1_score:.2f}'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plotHistory(axes, data, history):\n",
    "    anomaly_percentage = f'{data[\"anomaly_percentage\"]:1.2%}'\n",
    "    plt1 = axes[0,0]\n",
    "    plt1.plot(history['loss'], label=anomaly_percentage)\n",
    "    plt1.set_title(f'Training Loss')\n",
    "    plt1.set_ylabel('Loss')\n",
    "    \n",
    "    plt2 = axes[0,1]\n",
    "    plt2.plot(history['val_loss'], label=anomaly_percentage)\n",
    "    plt2.set_title(f'Validation Loss')\n",
    "    \n",
    "    plt3 = axes[1,0]\n",
    "    plt3.plot(history['binary_accuracy'], label=anomaly_percentage)\n",
    "    plt3.set_title(f'Training Accuracy')\n",
    "    plt3.set_ylabel('Accuracy')\n",
    "    \n",
    "    plt4 = axes[1,1]\n",
    "    plt4.plot(history['val_binary_accuracy'], label=anomaly_percentage)\n",
    "    plt4.set_title(f'Validation Accuracy')\n",
    "        \n",
    "\n",
    "def plotConfusionMatrix(matrix):\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=['Normal', 'Anomaly'])\n",
    "    disp.plot()\n",
    "\n",
    "def plotMetrics(metrics: list):\n",
    "    fig, axes = plt.subplots(2, 2, sharex=True, sharey='row')\n",
    "    fig.supxlabel('Epoch')\n",
    "\n",
    "    for  metric in metrics:\n",
    "        plotHistory(axes, metric['merged'], metric['history'])\n",
    "        plotConfusionMatrix(metric['results'].cm)\n",
    "        \n",
    "    handles, labels = fig.axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper center',title='Anomaly Percentage' )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
